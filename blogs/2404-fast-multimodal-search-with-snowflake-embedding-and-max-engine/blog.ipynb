{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks has the supporting code of our blog post. Make sure to read the included `README` in this sub-repository for dependency installations.\n",
    "\n",
    "Please see the inference section as we provide pretrained checkpoints for inference.\n",
    "\n",
    "## Data preparation and inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "class Flickr30kDataset(Dataset):\n",
    "    def __init__(self, split=\"train\", transform=None):\n",
    "        self.dataset = self._prepare_data(split)\n",
    "        self.transform = transform\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_data(split):\n",
    "        if split not in (\"train\", \"val\", \"test\"):\n",
    "            raise ValueError(f\"split must be either 'train', 'val' or 'test'. Given {split}\")\n",
    "\n",
    "        dataset = load_dataset(\"nlphuji/flickr30k\")[\"test\"]\n",
    "        dataset = dataset.remove_columns([\"sentids\", \"img_id\", \"filename\"])\n",
    "        return dataset.filter(lambda example: example[\"split\"] == split)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item['image']\n",
    "        captions = item['caption']\n",
    "        caption = random.choice(captions)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, caption\n",
    "\n",
    "# Image transformations for MobileNetV2 trained on imagenet-1k\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = Flickr30kDataset(split='train', transform=transform)\n",
    "val_dataset = Flickr30kDataset(split='val', transform=transform)\n",
    "test_dataset = Flickr30kDataset(split='test', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import fiftyone as fo\n",
    "\n",
    "def create_filckr32k_dashboard(split, max_samples=25, with_predictions=False, top_k=5, top_k_indices=None):\n",
    "    fo_dataset = fo.Dataset(f\"flickr30k-{split}\", overwrite=True)\n",
    "    samples = []\n",
    "    raw_dataset = Flickr30kDataset(split=split, transform=None)\n",
    "    save_path = f\"./{split}_images\"\n",
    "    save_path = Path(save_path)\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    for i, (image, caption) in enumerate(raw_dataset):\n",
    "        image = image.convert(\"RGB\")\n",
    "        filepath = str(save_path / f\"{i}.png\")\n",
    "        image.save(filepath)\n",
    "        sample = fo.Sample(filepath=filepath)\n",
    "        sample[\"ground_truth\"] = fo.Classification(label=caption)\n",
    "        if with_predictions:\n",
    "            assert top_k_indices is not None\n",
    "            for idx in range(top_k):\n",
    "                matching_caption_idx = top_k_indices[i][idx].item()\n",
    "                _, matching_caption = raw_dataset[matching_caption_idx]\n",
    "                sample[f\"predictions_{idx}\"] = fo.Classification(label=matching_caption)\n",
    "\n",
    "        samples.append(sample)\n",
    "\n",
    "        if i > max_samples: break\n",
    "\n",
    "    fo_dataset.add_samples(samples)\n",
    "    return fo_dataset\n",
    "\n",
    "fo_train_dataset = create_filckr32k_dashboard(split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fosess = fo.launch_app(fo_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fosess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train our custom multimodal search model\n",
    "\n",
    "### Pretrained text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('Snowflake/snowflake-arctic-embed-m')\n",
    "text_model = AutoModel.from_pretrained('Snowflake/snowflake-arctic-embed-m', add_pooling_layer=False)\n",
    "text_model.eval()\n",
    "\n",
    "def text_embeddings(texts):\n",
    "    tokens = tokenizer(list(texts), padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    with torch.no_grad():\n",
    "        query_embeddings = text_model(**tokens)[0][:, 0]\n",
    "\n",
    "    normalized = F.normalize(query_embeddings, p=2, dim=1)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained image embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import MobileNetV2ForImageClassification\n",
    "\n",
    "image_model = MobileNetV2ForImageClassification.from_pretrained(\n",
    "    \"google/mobilenet_v2_1.0_224\"\n",
    ")\n",
    "image_model.eval()\n",
    "def image_embeddings(images):\n",
    "    with torch.no_grad():\n",
    "        image_logits = image_model(images).logits\n",
    "\n",
    "    normalized = F.normalize(image_logits, p=2, dim=1)\n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Multimodal model and contrastive loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomMultiModalModel(nn.Module):\n",
    "    def __init__(self, text_embedding_dim=768, image_embedding_dim=1001, common_dim=64):\n",
    "        super().__init__()\n",
    "        self.text_transform = nn.Sequential(\n",
    "            nn.Linear(text_embedding_dim, common_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.image_transform = nn.Sequential(\n",
    "            nn.Linear(image_embedding_dim, common_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.final = nn.Linear(common_dim, common_dim)\n",
    "\n",
    "    def forward(self, image_embeddings, text_embeddings):\n",
    "        transformed_image = self.image_transform(image_embeddings)\n",
    "        transformed_text = self.text_transform(text_embeddings)\n",
    "        project_image = self.final(transformed_image)\n",
    "        project_text = self.final(transformed_text)\n",
    "        return project_image, project_text\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cosine_similarity = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    def forward(self, image_features, text_features):\n",
    "        logits = self.cosine_similarity(image_features.unsqueeze(0), text_features.unsqueeze(1)) / self.temperature\n",
    "        batch_size = text_features.shape[0]\n",
    "        # Labels are the main diagonal, as corresponding text and images are aligned\n",
    "        labels = torch.arange(batch_size).to(logits.device)\n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n",
    "\n",
    "lr = 3e-4\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = CustomMultiModalModel()\n",
    "loss_fn = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for image_batch, text_batch in train_loader:\n",
    "        image_features = image_embeddings(image_batch)\n",
    "        text_features = text_embeddings(text_batch)\n",
    "        optimizer.zero_grad()\n",
    "        image_out, text_out = model(image_features, text_features)\n",
    "        train_loss = loss_fn(image_out, text_out)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(train_loss.item())\n",
    "        print(f\"Epoch {epoch}, Loss: {train_loss.item()}\")\n",
    "\n",
    "    average_train_loss = sum(train_losses) / len(train_losses)\n",
    "    train_loss_history.append(average_train_loss)\n",
    "    print(f\"Epoch {epoch}, Average train loss: {average_train_loss}\")\n",
    "\n",
    "    torch.save(model.state_dict(), f\"custom_multimodal_model_epoch_{epoch}.pt\")\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for image_batch, text_batch in val_loader:\n",
    "            image_features = image_embeddings(image_batch)\n",
    "            text_features = text_embeddings(text_batch)\n",
    "            image_out, text_out = model(image_features, text_features)\n",
    "            val_loss = loss_fn(image_out, text_out)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "    average_val_loss = sum(val_losses) / len(val_losses)\n",
    "    val_loss_history.append(average_val_loss)\n",
    "    print(f\"Epoch {epoch}, Average validation loss: {average_val_loss}\")\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_epochs = range(1, len(train_loss_history) + 1)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(plot_epochs, train_loss_history, marker='o', linestyle='-', color='b', label='Training Loss')\n",
    "plt.plot(plot_epochs, val_loss_history, marker='o', linestyle='-', color='r', label='Validation Loss')\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xticks(plot_epochs)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "### Convert to TorchScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "example_image_inputs = torch.zeros((1, 3, 224, 224))\n",
    "with torch.no_grad():\n",
    "    traced_image_model = torch.jit.trace(image_model, example_image_inputs, strict=False)\n",
    "\n",
    "traced_image_model.save(\"image_embedding.torchscript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text_inputs = {\n",
    "    \"input_ids\": torch.zeros((1, 512), dtype=torch.long),\n",
    "    \"token_type_ids\": torch.zeros((1, 512), dtype=torch.long),\n",
    "    \"attention_mask\": torch.zeros((1, 512), dtype=torch.long),\n",
    "}\n",
    "with torch.no_grad():\n",
    "    traced_text_model = torch.jit.trace(text_model, example_kwarg_inputs=dict(example_text_inputs), strict=False)\n",
    "\n",
    "traced_text_model.save(\"text_embedding.torchscript\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomMultiModalModel()\n",
    "model.load_state_dict(torch.load(\"custom_multimodal_model_epoch_2.pt\"))\n",
    "model.eval()\n",
    "example_image_embeddings = torch.rand(1, 1001)\n",
    "example_text_embeddings = torch.rand(1, 768)\n",
    "\n",
    "with torch.no_grad():\n",
    "    traced_script_module = torch.jit.trace(model, (example_image_embeddings, example_text_embeddings))\n",
    "\n",
    "traced_script_module.save(\"custom_multimodal_model.torchscript\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and compile TorchScript models in MAX Engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from max import engine\n",
    "\n",
    "session = engine.InferenceSession()\n",
    "\n",
    "max_image_embedding = session.load(\"image_embedding.torchscript\", engine.TorchLoadOptions([\n",
    "    engine.TorchInputSpec(shape=[None, 3, 224, 224], dtype=engine.DType.float32)\n",
    "]))\n",
    "max_text_embedding = session.load(\"text_embedding.torchscript\", engine.TorchLoadOptions([\n",
    "    engine.TorchInputSpec(shape=[None, None], dtype=engine.DType.int64),\n",
    "    engine.TorchInputSpec(shape=[None, None], dtype=engine.DType.int64),\n",
    "    engine.TorchInputSpec(shape=[None, None], dtype=engine.DType.int64),\n",
    "]))\n",
    "\n",
    "max_custom_multimodal = session.load(\"custom_multimodal_model.torchscript\", engine.TorchLoadOptions([\n",
    "    engine.TorchInputSpec(shape=[None, 1001], dtype=engine.DType.float32),\n",
    "    engine.TorchInputSpec(shape=[None, 768], dtype=engine.DType.float32),\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine input and output metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_image_embedding.input_metadata)\n",
    "print(max_image_embedding.output_metadata)\n",
    "print(\"=\"*80)\n",
    "print(max_text_embedding.input_metadata)\n",
    "print(max_text_embedding.output_metadata)\n",
    "print(\"=\"*80)\n",
    "print(max_custom_multimodal.input_metadata)\n",
    "print(max_custom_multimodal.output_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "all_image_embeddings = []\n",
    "all_text_embeddings = []\n",
    "for image, caption in test_loader:\n",
    "    img_emb = max_image_embedding.execute(pixel_values=image)[\"result0\"][\"logits\"]\n",
    "    tokenized_caption = tokenizer(caption, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    txt_emb = max_text_embedding.execute(**tokenized_caption)[\"result0\"][\"last_hidden_state\"][:, 0, :]\n",
    "\n",
    "    ret = max_custom_multimodal.execute(image_embeddings=img_emb, text_embeddings=txt_emb)\n",
    "    projected_image_emb, projected_text_emb = ret[\"result0\"], ret[\"result1\"]\n",
    "\n",
    "    all_image_embeddings.append(torch.from_numpy(projected_image_emb))\n",
    "    all_text_embeddings.append(torch.from_numpy(projected_text_emb))\n",
    "\n",
    "assert len(all_image_embeddings) == len(all_text_embeddings)\n",
    "\n",
    "all_image_embeddings = torch.cat(all_image_embeddings, dim=0)\n",
    "all_text_embeddings = torch.cat(all_text_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import torch.nn.functional as F\n",
    "cosine_similarities = F.cosine_similarity(all_image_embeddings.unsqueeze(1), all_text_embeddings.unsqueeze(0), dim=2)\n",
    "\n",
    "k = 5\n",
    "top_k_values, top_k_indices = torch.topk(cosine_similarities, k=k, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fo_test_dataset = create_filckr32k_dashboard(split=\"test\", with_predictions=True, top_k=k, top_k_indices=top_k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fosess = fo.launch_app(fo_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fosess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
